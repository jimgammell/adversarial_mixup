{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63dc600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn.utils import spectral_norm\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96205289",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, use_bn=True, use_sn=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        modules = []\n",
    "        modules.append(nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2, bias=not use_bn))\n",
    "        if use_bn:\n",
    "            modules.append(nn.BatchNorm2d(6))\n",
    "        modules.append(nn.ReLU(inplace=True))\n",
    "        modules.append(nn.AvgPool2d(2))\n",
    "        modules.append(nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=2, bias=not use_bn))\n",
    "        if use_bn:\n",
    "            modules.append(nn.BatchNorm2d(16))\n",
    "        modules.append(nn.ReLU(inplace=True))\n",
    "        modules.append(nn.AvgPool2d(2))\n",
    "        modules.append(nn.Flatten())\n",
    "        modules.append(nn.Linear(784, 120))\n",
    "        modules.append(nn.ReLU(inplace=True))\n",
    "        modules.append(nn.Linear(120, 84))\n",
    "        modules.append(nn.ReLU(inplace=True))\n",
    "        if use_sn:\n",
    "            for m_idx, m in enumerate(modules):\n",
    "                if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "                    modules[m_idx] = spectral_norm(m)\n",
    "        \n",
    "        self.fe = nn.Sequential(*modules)\n",
    "        self.classifier_head = nn.Linear(84, 10)\n",
    "        self.class_embedding = nn.Linear(10, 84)\n",
    "        self.critic_head = nn.Linear(84, 1)\n",
    "        if use_sn:\n",
    "            self.classifier_head = spectral_norm(self.classifier_head)\n",
    "            self.class_embedding = spectral_norm(self.class_embedding)\n",
    "            self.critic_head = spectral_norm(self.critic_head)\n",
    "        \n",
    "    def criticize_example(self, x, y):\n",
    "        x_fe = self.fe(x)\n",
    "        embedded_y = self.class_embedding(y)\n",
    "        out = self.critic_head(x_fe) + (x_fe * embedded_y).sum(dim=1)\n",
    "        return out\n",
    "    \n",
    "    def classify_example(self, x):\n",
    "        x_fe = self.fe(x)\n",
    "        out = self.classifier_head(x_fe)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecbea07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_loss(logits, y):\n",
    "    return nn.functional.relu(1 - y*logits).mean()\n",
    "\n",
    "def extrapolate_example(x, y_target, model, no_grad=False):\n",
    "    def loss_fn(x):\n",
    "        return nn.functional.cross_entropy(model.classify_example(x), y_target)\n",
    "    x_g = x.clone().requires_grad_(True)\n",
    "    print(x_g.is_leaf)\n",
    "    loss = loss_fn(x_g)\n",
    "    loss.backward(inputs=x_g, create_graph=not no_grad)\n",
    "    delta_x = x_g.grad\n",
    "    alpha = torch.zeros(x.size(0), 1, 1, 1, device=x.device, requires_grad=True)\n",
    "    alpha_opt = optim.LBFGS([alpha.data], line_search='strong_wolfe')\n",
    "    alpha_opt.zero_grad()\n",
    "    alpha_opt.step(lambda: loss_fn(nn.functional.tanh(x+alpha*delta_x.detach())))\n",
    "    alpha_opt = alpha.detach()\n",
    "    return delta_x, alpha_opt\n",
    "\n",
    "def train_step(batch, classifiers, optimizers, device, single_purpose_models=True):\n",
    "    x, y = batch\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    if single_purpose_models:\n",
    "        critic_idx = 0\n",
    "    else:\n",
    "        critic_idx = np.random.randint(2)\n",
    "    critic, critic_opt = classifiers[critic_idx], optimizers[critic_idx]\n",
    "    classifier, classifier_opt = classifiers[1-critic_idx], optimizers[1-critic_idx]\n",
    "    y_target = torch.randint_like(y, 10)\n",
    "    \n",
    "    # update critic\n",
    "    delta_x, alpha_opt = extrapolate_example(x, y_target, classifier, no_grad=True)\n",
    "    critic_logits_fake = critic.criticize_example(nn.functional.tanh(x+alpha_opt*delta_x), y_target)\n",
    "    critic_logits_real = critic.criticize_example(x, y)\n",
    "    critic_loss = 0.5*hinge_loss(critic_logits_fake, -1) + 0.5*hinge_loss(critic_logits_real, 1)\n",
    "    critic_opt.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    critic_opt.step()\n",
    "    \n",
    "    # update classifier\n",
    "    delta_x, alpha_opt = extrapolate_example(x, y_target, classifier)\n",
    "    critic_logits = critic.criticize_example(nn.functional.tanh(x + alpha_opt*delta_x), y_target)\n",
    "    realism_loss = -critic_logits.mean()\n",
    "    alpha = alpha_opt*torch.rand_like(alpha_opt)\n",
    "    classifier_logits = classifier.classify_example(alpha*x + (1-alpha)*nn.functional.tanh(x+alpha_opt*delta_x))\n",
    "    classifier_loss = ((alpha_opt-alpha)/alpha_opt)*nn.functional.cross_entropy(classifier_logits, y) + (alpha/alpha_opt)*nn.functional.cross_entropy(classifier_logits, y_target)\n",
    "    loss = 0.5*realism_loss + 0.5*classifier_loss\n",
    "    classifier_opt.zero_grad()\n",
    "    classifier_loss.backward()\n",
    "    classifier_opt.step()\n",
    "    \n",
    "def eval_step(batch, classifiers, device, single_purpose_models=True):\n",
    "    x, y = batch\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    if single_purpose_models:\n",
    "        critic_idx = 0\n",
    "    else:\n",
    "        critic_idx = np.random.randint(2)\n",
    "    critic = classifiers[critic_idx]\n",
    "    classifier = classifiers[1-critic_idx]\n",
    "    y_target = torch.randint_like(y, 10)\n",
    "    \n",
    "    delta_x, alpha_opt = extrapolate_example(x, y_target, classifier, no_grad=True)\n",
    "    with torch.no_grad():\n",
    "        critic_logits_fake = critic.criticize_example(x + alpha_opt*delta_x, y_target)\n",
    "        critic_logits_real = critic.criticize_example(x, y)\n",
    "        critic_loss = 0.5*hinge_loss(critic_logits_fake, -1) + 0.5*hinge_loss(critic_logits_real, 1)\n",
    "        realism_loss = -critic_logits_fake.mean()\n",
    "        classifier_logits = classifier.classify_example(x)\n",
    "        classifier_loss = nn.functional.cross_entropy(classifier_logits, y)\n",
    "        classifier_acc = np.mean(np.equal(np.argmax(classifier_logits.detach().cpu().numpy()), y))\n",
    "    \n",
    "    return {\n",
    "        'critic_loss': critic_loss.detach().cpu().numpy(),\n",
    "        'realism_loss': realism_loss.detach().cpu().numpy(),\n",
    "        'classifier_loss': classifier_loss.detach().cpu().numpy(),\n",
    "        'classifier_acc': classifier_acc,\n",
    "        'alpha_opt': alpha_opt\n",
    "    }\n",
    "\n",
    "def run_epoch(train_dataloader, test_dataloader, classifiers, optimizers, device, **step_kwargs):\n",
    "    for batch in train_dataloader:\n",
    "        train_step(batch, classifiers, optimizers, device, **step_kwargs)\n",
    "    rv = {}\n",
    "    for batch in test_dataloader:\n",
    "        step_rv = eval_step(batch, classifiers, device, **step_kwargs)\n",
    "        for key, item in step_rv.items():\n",
    "            if not key in rv.keys():\n",
    "                rv[key] = []\n",
    "            rv[key].append(item)\n",
    "    for key, item in rv.items():\n",
    "        rv[key] = np.mean(item)\n",
    "        \n",
    "    x, y = next(iter(test_dataloader))\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    y_target = torch.tensor([i for i in range(10) for _ in range(len(y)//10)], device=device, dtype=torch.long)\n",
    "    delta_x, alpha_opt = extrapolate_example(x[:len(y_target)], y_target, model, no_grad=True)\n",
    "    rv['reference_images'] = x[:len(y_target)].cpu().numpy()\n",
    "    rv['generated_images'] = (x + alpha_opt*delta_x).detach().cpu().numpy()\n",
    "    return rv\n",
    "\n",
    "def display_results(rv):\n",
    "    for key, item in rv.items():\n",
    "        if not hasattr(item, '__len__'):\n",
    "            print('{}: {}'.format(key, item))\n",
    "        else:\n",
    "            num_cols = int(np.sqrt(len(item)))\n",
    "            num_rows = num_rows + int(len(item) > num_cols**2)\n",
    "            (fig, axes) = plt.subplots(num_rows, num_cols, figsize=(4*num_cols, 4*num_rows))\n",
    "            for x, ax in zip(item, axes.flatten()):\n",
    "                ax.imshow(x)\n",
    "            fig.suptitle(key)\n",
    "    plt.show()\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd93ee9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "classifier = LeNet5(use_bn=True, use_sn=False).to(device)\n",
    "critic = LeNet5(use_bn=False, use_sn=True).to(device)\n",
    "classifier_opt = optim.Adam(classifier.parameters(), lr=1e-5, betas=(0.5, 0.999))\n",
    "critic_opt = optim.Adam(critic.parameters(), lr=5e-5, betas=(0.5, 0.999))\n",
    "batch_size = 64\n",
    "train_dataset = torchvision.datasets.MNIST(root=os.path.join('.', 'downloads'), train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root=os.path.join('.', 'downloads'), train=False, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f55b70d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "One of the differentiated Tensors given as 'inputs' to backward is not a leaf Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2930129/1691080013.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mrv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcritic_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier_opt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msingle_purpose_models\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdisplay_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2930129/201826043.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(train_dataloader, test_dataloader, classifiers, optimizers, device, **step_kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mstep_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mstep_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0mrv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2930129/201826043.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(batch, classifiers, optimizers, device, single_purpose_models)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# update critic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mdelta_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextrapolate_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mcritic_logits_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriticize_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha_opt\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdelta_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mcritic_logits_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriticize_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2930129/201826043.py\u001b[0m in \u001b[0;36mextrapolate_example\u001b[0;34m(x, y_target, model, no_grad)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mx_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_g\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mno_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mdelta_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sca_defense/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sca_defense/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: One of the differentiated Tensors given as 'inputs' to backward is not a leaf Tensor"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    rv = run_epoch(train_dataloader, test_dataloader, [critic, classifier], [critic_opt, classifier_opt], device, single_purpose_models=True)\n",
    "    display_results(rv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
